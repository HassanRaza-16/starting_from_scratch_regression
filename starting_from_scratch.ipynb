{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18fdc83-442c-49ae-9e79-75c6159e06b5",
   "metadata": {},
   "source": [
    "## STARTING FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c3514-bcef-4ef5-b2a2-5f0fa5035a25",
   "metadata": {},
   "source": [
    "## GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e6b5f-3c42-46c1-b8be-4081c97c30ec",
   "metadata": {},
   "source": [
    "## So two ways here to start from scratch one is to make class and them implement and use .fit on dataset other is to implement direct  in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ed734-64fb-4233-8222-80ee9f3ea1ac",
   "metadata": {},
   "source": [
    "# 1. Directly implement in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5216eecc-3ae3-4ccf-a5d7-29682e3b33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb35632f-6a4b-4f89-a904-69f48efd7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f27fc836-1a2b-465b-88d8-27f9eac0d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term (column of 1s)\n",
    "X_b = np.c_[np.ones(len(X)), X]    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e43110ef-4e44-4195-8881-b108e0ba566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters [theta_0 (bias), theta_1 (weight)] with zeros\n",
    "theta = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d38e4f57-5d83-4aea-8489-dfd4a9fb28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate (step size) and epochs (number of training loops)\n",
    "# You can adjust these based on how fast and well the model is learning\n",
    "learning_rate = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99dc6867-1393-4546-90cc-8fb9205d913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (weights): [0.01740046 1.99518035]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent Loop\n",
    "for i in range(epochs):\n",
    "    predictions = X_b.dot(theta)\n",
    "    error = predictions - y\n",
    "    gradients = (2/len(X)) * X_b.T.dot(error)\n",
    "    theta -= learning_rate * gradients\n",
    "print(\"Theta (weights):\", theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e3a75dc-d8ae-4fa0-9ff0-1392bf0e83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (weights): [0.01740046 1.99518035]\n"
     ]
    }
   ],
   "source": [
    "# Final learned weights: theta[0] is bias (intercept), theta[1] is weight (slope)\n",
    "# These values define the best-fit line: y = theta[0] + theta[1] * x\n",
    "print(\"Theta (weights):\", theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06bda9f-49aa-4fd4-8a00-435063cdee3b",
   "metadata": {},
   "source": [
    "# 2.Using .fit by making class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb5f7b2d-12fb-4f09-a641-dd22398f7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.theta = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones(len(X)), X]    #In your class version, the number of features might change depending on the dataset â€” you want it to be dynamic.\n",
    "        self.theta = np.zeros(X_b.shape[1])\n",
    "        for _ in range(self.epochs):\n",
    "            predictions = X_b.dot(self.theta)\n",
    "            error = predictions - y\n",
    "            gradients = (2/len(X)) * X_b.T.dot(error)\n",
    "            self.theta -= self.learning_rate * gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones(len(X)), X]\n",
    "        return X_b.dot(self.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d929e12-6805-4ba8-a12a-249fb5cd8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72906c6a-719e-402e-8370-51afa94d4916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.01258081, 4.00776116, 6.00294152, 7.99812187, 9.99330222])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348be31c-199b-435d-a170-53d15be91e65",
   "metadata": {},
   "source": [
    "# 2. STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91688565-ca6d-4000-9277-1c779dd1e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c42f5030-2ba5-4213-9239-17f34012f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (weights): [0.39863458 1.90597914]\n"
     ]
    }
   ],
   "source": [
    "# Add bias\n",
    "X_b = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.zeros(2)\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "# SGD Loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        xi = X_b[i:i+1]\n",
    "        yi = y[i]\n",
    "        prediction = xi.dot(theta)\n",
    "        error = prediction - yi\n",
    "        gradients = 2 * xi.T.dot(error)\n",
    "        theta -= learning_rate * gradients\n",
    "\n",
    "print(\"Theta (weights):\", theta)        ##Gradient Descent uses the full dataset per update (with 2D arrays), while Stochastic Gradient Descent updates after each sample using a 2D slice (1 row) to keep dot product math correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f12711-8e4c-4674-85b8-72c3e3e7f7b3",
   "metadata": {},
   "source": [
    "# 3.NORMAL EQUATION (NO ITERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1018c38f-f6e5-4aa6-812e-8ec610dd7df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (weights): [5.66213743e-15 2.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Add bias\n",
    "X_b = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "# Normal Equation\n",
    "theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(\"Theta (weights):\", theta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranch",
   "language": "python",
   "name": "ranch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
